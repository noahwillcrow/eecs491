{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By: Noah Crowley\n",
    "\n",
    "Case ID: nwc17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem\n",
    "\n",
    "An Iowa police officer has arrived at a crash scene and wants to determine the likely cause of the accident before hearing any explanations from those involved. The officer only has the following information:\n",
    "\n",
    "- G<sub>i</sub> - The age group of driver i\n",
    "  - [1] &lt;18\n",
    "  - [2] 18-25\n",
    "  - [3] 26-64\n",
    "  - [4] &gt;64\n",
    "- I<sub>i</sub> - Whether driver i was in any way impaired (alcohol, drugs, simple vision impairment)\n",
    "  - True\n",
    "  - False\n",
    "- N<sub>i</sub> - The number of occupants in vehicle i\n",
    "  - Any nonnegative integer\n",
    "- T - The presence of any traffic controls (stop light, stop sign, etc.) in the immediate area of the accident\n",
    "  - True\n",
    "  - False\n",
    "- C - The road conditions\n",
    "  - [1] Normal condition\n",
    "  - [2] Wet\n",
    "  - [3] Ice/Frost\n",
    "  - [4] Snow\n",
    "  - [5] Slush\n",
    "  - [6] Mud\n",
    "  - [7] Flooded\n",
    "  - [8] Sand\n",
    "  - [9] Oil\n",
    "  - [10] Gravel\n",
    "  - [98] Other\n",
    "  - [99] Unknown\n",
    "- S - The severity rating of the accident\n",
    "  - [1] Fatal\n",
    "  - [2] Suspected serious injuries\n",
    "  - [3] Suspected minor injuries\n",
    "  - [4] Possible injury\n",
    "  - [5] Uninjured\n",
    "  - [7] Fatal, not crash related\n",
    "  - [9] Unknown\n",
    "- F - The number of fatalities that have resulted from the accident\n",
    "  - Any nonnegative integer\n",
    "\n",
    "Using this information and the Iowa Office of Traffic & Safety's crash database, the officer will infer the likelihood of the individual vehicle actions (A) that led to the accident, which can take on the following values:\n",
    "\n",
    "- [1] Moving straight, staying in lane\n",
    "- [2] Turning left\n",
    "- [3] Turning right\n",
    "- [4] Making a u-turn\n",
    "- [5] Overtaking\n",
    "- [6] Changing lanes\n",
    "- [7] Merging into traffic\n",
    "- [8] Exiting from traffic\n",
    "- [9] Going in reverse on roadway (not for use with parking spaces)\n",
    "- [10] Slowing/Stopping\n",
    "- [11] Stopped in traffic (e.g. stopped for a stop sign)\n",
    "- [12] Legally parked\n",
    "- [13] Illegally parked\n",
    "- [88] Other\n",
    "- [99] Unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Network Diagram.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior Probabilities\n",
    "\n",
    "With the Iowa dataset, the officer already knows generic priors about all of these variables and their distributions over the past ten years. _(This data was collected in the Data Analysis notebook, which is being kept separate as it is very large and mostly not pertinent to the core discussion)_ Below are the results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"output output_scroll\"><div class=\"output_area\"><div class=\"prompt\"></div><div class=\"output_subarea output_html rendered_html\"><h4>Age group</h4><br><table width=\"30%\"><thead><tr><td>Value</td><td>Probability</td></tr></thead><tbody><tr><td>1</td><td>0.069</td></tr><tr><td>2</td><td>0.214</td></tr><tr><td>3</td><td>0.543</td></tr><tr><td>4</td><td>0.174</td></tr></tbody></table></div></div><div class=\"output_area\"><div class=\"prompt\"></div><div class=\"output_subarea output_html rendered_html\"><h4>Number of occupants</h4><br><table width=\"30%\"><thead><tr><td>Value</td><td>Probability</td></tr></thead><tbody><tr><td>0.0</td><td>0.049</td></tr><tr><td>1.0</td><td>0.689</td></tr><tr><td>2.0</td><td>0.182</td></tr><tr><td>3.0</td><td>0.050</td></tr><tr><td>4.0</td><td>0.021</td></tr><tr><td>5.0</td><td>0.007</td></tr><tr><td>6.0</td><td>0.002</td></tr></tbody></table></div></div><div class=\"output_area\"><div class=\"prompt\"></div><div class=\"output_subarea output_html rendered_html\"><h4>Impaired</h4><br><table width=\"30%\"><thead><tr><td>Value</td><td>Probability</td></tr></thead><tbody><tr><td>False</td><td>0.772</td></tr><tr><td>True</td><td>0.228</td></tr></tbody></table></div></div><div class=\"output_area\"><div class=\"prompt\"></div><div class=\"output_subarea output_html rendered_html\"><h4>Had traffic control</h4><br><table width=\"30%\"><thead><tr><td>Value</td><td>Probability</td></tr></thead><tbody><tr><td>False</td><td>0.609</td></tr><tr><td>True</td><td>0.391</td></tr></tbody></table></div></div><div class=\"output_area\"><div class=\"prompt\"></div><div class=\"output_subarea output_html rendered_html\"><h4>Surface condition</h4><br><table width=\"30%\"><thead><tr><td>Value</td><td>Probability</td></tr></thead><tbody><tr><td>1.0</td><td>0.658</td></tr><tr><td>2.0</td><td>0.128</td></tr><tr><td>3.0</td><td>0.057</td></tr><tr><td>4.0</td><td>0.077</td></tr><tr><td>5.0</td><td>0.013</td></tr><tr><td>6.0</td><td>0.014</td></tr><tr><td>10.0</td><td>0.002</td></tr><tr><td>77.0</td><td>0.035</td></tr><tr><td>99.0</td><td>0.013</td></tr><tr><td>98.0</td><td>0.003</td></tr></tbody></table></div></div><div class=\"output_area\"><div class=\"prompt\"></div><div class=\"output_subarea output_html rendered_html\"><h4>Action</h4><br><table width=\"30%\"><thead><tr><td>Value</td><td>Probability</td></tr></thead><tbody><tr><td>1.0</td><td>0.585</td></tr><tr><td>2.0</td><td>0.094</td></tr><tr><td>3.0</td><td>0.037</td></tr><tr><td>4.0</td><td>0.003</td></tr><tr><td>5.0</td><td>0.007</td></tr><tr><td>6.0</td><td>0.017</td></tr><tr><td>7.0</td><td>0.008</td></tr><tr><td>8.0</td><td>0.002</td></tr><tr><td>9.0</td><td>0.025</td></tr><tr><td>10.0</td><td>0.067</td></tr><tr><td>11.0</td><td>0.047</td></tr><tr><td>12.0</td><td>0.044</td></tr><tr><td>13.0</td><td>0.003</td></tr><tr><td>14.0</td><td>0.002</td></tr><tr><td>99.0</td><td>0.016</td></tr><tr><td>77.0</td><td>0.028</td></tr><tr><td>98.0</td><td>0.017</td></tr></tbody></table></div></div><div class=\"output_area\"><div class=\"prompt\"></div><div class=\"output_subarea output_html rendered_html\"><h4>Severity</h4><br><table width=\"30%\"><thead><tr><td>Value</td><td>Probability</td></tr></thead><tbody><tr><td>1.0</td><td>0.006</td></tr><tr><td>2.0</td><td>0.023</td></tr><tr><td>3.0</td><td>0.089</td></tr><tr><td>4.0</td><td>0.172</td></tr><tr><td>5.0</td><td>0.710</td></tr></tbody></table></div></div><div class=\"output_area\"><div class=\"prompt\"></div><div class=\"output_subarea output_html rendered_html\"><h4>Number of fatalities</h4><br><table width=\"30%\"><thead><tr><td>Value</td><td>Probability</td></tr></thead><tbody><tr><td>0.0</td><td>0.995</td></tr><tr><td>1.0</td><td>0.005</td></tr></tbody></table></div></div></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model\n",
    "\n",
    "In order to make these priors useful, they need to be put into a model and then put to use. This will be done in Python and using the results from the Data Analysis notebook, which have been saved in a csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pgmpy.models import BayesianModel as bayesmodel\n",
    "from pgmpy.factors.discrete import TabularCPD as tcpd\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pandas.read_csv(\"relevant_iowa_crash_data.csv\", delimiter = \",\", header = 0,\n",
    "                                  error_bad_lines = False, dtype = {\n",
    "                                      \"Age group\": int,\n",
    "                                      \"Number of occupants\": int,\n",
    "                                      \"Impaired\": str,\n",
    "                                      \"Had traffic control\": str,\n",
    "                                      \"Surface condition\": int,\n",
    "                                      \"Action\": int,\n",
    "                                      \"Severity\": int,\n",
    "                                      \"Number of fatalities\": int\n",
    "                                  })\n",
    "df.rename(index=str, columns={\n",
    "    \"Age group\": \"G\", \"Number of occupants\": \"N\", \"Impaired\": \"I\",\n",
    "    \"Had traffic control\": \"T\", \"Surface condition\": \"C\",\n",
    "    \"Action\": \"A\", \"Severity\": \"S\", \"Number of fatalities\": \"F\"\n",
    "}, inplace=True)\n",
    "\n",
    "model = bayesmodel([\n",
    "    (\"G\", \"A\"), (\"T\", \"A\"), (\"C\", \"A\"), (\"I\", \"A\"), (\"N\", \"A\"),\n",
    "    (\"I\", \"S\"), (\"A\", \"S\"), (\"S\", \"F\"), (\"N\", \"F\")\n",
    "])\n",
    "model.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.check_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the problem\n",
    "\n",
    "Now that the model has been created, it is time to put it to work and use it to predict make predictions.\n",
    "\n",
    "For a demo, the following evidence variables will be used:\n",
    "\n",
    "- G<sub>1</sub>: 2\n",
    "- I<sub>1</sub>: False\n",
    "- N<sub>1</sub>: 1\n",
    "- G<sub>2</sub>: 3\n",
    "- I<sub>2</sub>: True\n",
    "- N<sub>2</sub>: 2\n",
    "- T: True\n",
    "- C: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_variables = [\"A\",\"S\",\"F\"]\n",
    "v1_evidence = {\"G\":2,\"N\":1,\"I\":0,\"C\":1,\"T\":1}\n",
    "v2_evidence = {\"G\":3,\"N\":2,\"I\":1,\"C\":1,\"T\":1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using variable elimination\n",
    "\n",
    "The first method to use for the model will be variable elimination. This method will be slow, but should provide the officer with a substitute for computing probabilities theoretically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pgmpy.inference import VariableElimination\n",
    "\n",
    "ve_solver = VariableElimination(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒══════╤══════════╕\n",
      "│ A    │   phi(A) │\n",
      "╞══════╪══════════╡\n",
      "│ A_0  │   0.5381 │\n",
      "├──────┼──────────┤\n",
      "│ A_1  │   0.1329 │\n",
      "├──────┼──────────┤\n",
      "│ A_2  │   0.0510 │\n",
      "├──────┼──────────┤\n",
      "│ A_3  │   0.0016 │\n",
      "├──────┼──────────┤\n",
      "│ A_4  │   0.0009 │\n",
      "├──────┼──────────┤\n",
      "│ A_5  │   0.0064 │\n",
      "├──────┼──────────┤\n",
      "│ A_6  │   0.0062 │\n",
      "├──────┼──────────┤\n",
      "│ A_7  │   0.0012 │\n",
      "├──────┼──────────┤\n",
      "│ A_8  │   0.0052 │\n",
      "├──────┼──────────┤\n",
      "│ A_9  │   0.0873 │\n",
      "├──────┼──────────┤\n",
      "│ A_10 │   0.1570 │\n",
      "├──────┼──────────┤\n",
      "│ A_11 │   0.0003 │\n",
      "├──────┼──────────┤\n",
      "│ A_12 │   0.0001 │\n",
      "├──────┼──────────┤\n",
      "│ A_13 │   0.0001 │\n",
      "├──────┼──────────┤\n",
      "│ A_14 │   0.0007 │\n",
      "├──────┼──────────┤\n",
      "│ A_15 │   0.0001 │\n",
      "├──────┼──────────┤\n",
      "│ A_16 │   0.0003 │\n",
      "├──────┼──────────┤\n",
      "│ A_17 │   0.0001 │\n",
      "├──────┼──────────┤\n",
      "│ A_18 │   0.0005 │\n",
      "├──────┼──────────┤\n",
      "│ A_19 │   0.0079 │\n",
      "├──────┼──────────┤\n",
      "│ A_20 │   0.0019 │\n",
      "╘══════╧══════════╛\n",
      "╒═════╤══════════╕\n",
      "│ S   │   phi(S) │\n",
      "╞═════╪══════════╡\n",
      "│ S_0 │   0.0045 │\n",
      "├─────┼──────────┤\n",
      "│ S_1 │   0.0227 │\n",
      "├─────┼──────────┤\n",
      "│ S_2 │   0.0936 │\n",
      "├─────┼──────────┤\n",
      "│ S_3 │   0.1933 │\n",
      "├─────┼──────────┤\n",
      "│ S_4 │   0.6858 │\n",
      "╘═════╧══════════╛\n",
      "╒═════╤══════════╕\n",
      "│ F   │   phi(F) │\n",
      "╞═════╪══════════╡\n",
      "│ F_0 │   0.9955 │\n",
      "├─────┼──────────┤\n",
      "│ F_1 │   0.0042 │\n",
      "├─────┼──────────┤\n",
      "│ F_2 │   0.0002 │\n",
      "├─────┼──────────┤\n",
      "│ F_3 │   0.0000 │\n",
      "├─────┼──────────┤\n",
      "│ F_4 │   0.0000 │\n",
      "├─────┼──────────┤\n",
      "│ F_5 │   0.0000 │\n",
      "╘═════╧══════════╛\n",
      "╒══════╤══════════╕\n",
      "│ A    │   phi(A) │\n",
      "╞══════╪══════════╡\n",
      "│ A_0  │   0.5000 │\n",
      "├──────┼──────────┤\n",
      "│ A_1  │   0.2063 │\n",
      "├──────┼──────────┤\n",
      "│ A_2  │   0.0238 │\n",
      "├──────┼──────────┤\n",
      "│ A_3  │   0.0000 │\n",
      "├──────┼──────────┤\n",
      "│ A_4  │   0.0000 │\n",
      "├──────┼──────────┤\n",
      "│ A_5  │   0.0000 │\n",
      "├──────┼──────────┤\n",
      "│ A_6  │   0.0238 │\n",
      "├──────┼──────────┤\n",
      "│ A_7  │   0.0000 │\n",
      "├──────┼──────────┤\n",
      "│ A_8  │   0.0159 │\n",
      "├──────┼──────────┤\n",
      "│ A_9  │   0.0635 │\n",
      "├──────┼──────────┤\n",
      "│ A_10 │   0.0556 │\n",
      "├──────┼──────────┤\n",
      "│ A_11 │   0.0317 │\n",
      "├──────┼──────────┤\n",
      "│ A_12 │   0.0000 │\n",
      "├──────┼──────────┤\n",
      "│ A_13 │   0.0000 │\n",
      "├──────┼──────────┤\n",
      "│ A_14 │   0.0079 │\n",
      "├──────┼──────────┤\n",
      "│ A_15 │   0.0000 │\n",
      "├──────┼──────────┤\n",
      "│ A_16 │   0.0000 │\n",
      "├──────┼──────────┤\n",
      "│ A_17 │   0.0000 │\n",
      "├──────┼──────────┤\n",
      "│ A_18 │   0.0238 │\n",
      "├──────┼──────────┤\n",
      "│ A_19 │   0.0079 │\n",
      "├──────┼──────────┤\n",
      "│ A_20 │   0.0397 │\n",
      "╘══════╧══════════╛\n",
      "╒═════╤══════════╕\n",
      "│ S   │   phi(S) │\n",
      "╞═════╪══════════╡\n",
      "│ S_0 │   0.0091 │\n",
      "├─────┼──────────┤\n",
      "│ S_1 │   0.0235 │\n",
      "├─────┼──────────┤\n",
      "│ S_2 │   0.0841 │\n",
      "├─────┼──────────┤\n",
      "│ S_3 │   0.1615 │\n",
      "├─────┼──────────┤\n",
      "│ S_4 │   0.7217 │\n",
      "╘═════╧══════════╛\n",
      "╒═════╤══════════╕\n",
      "│ F   │   phi(F) │\n",
      "╞═════╪══════════╡\n",
      "│ F_0 │   0.9909 │\n",
      "├─────┼──────────┤\n",
      "│ F_1 │   0.0073 │\n",
      "├─────┼──────────┤\n",
      "│ F_2 │   0.0017 │\n",
      "├─────┼──────────┤\n",
      "│ F_3 │   0.0001 │\n",
      "├─────┼──────────┤\n",
      "│ F_4 │   0.0000 │\n",
      "├─────┼──────────┤\n",
      "│ F_5 │   0.0000 │\n",
      "╘═════╧══════════╛\n"
     ]
    }
   ],
   "source": [
    "ve_res1 = ve_solver.query(query_variables, v1_evidence)\n",
    "print(ve_res1[\"A\"])\n",
    "print(ve_res1[\"S\"])\n",
    "print(ve_res1[\"F\"])\n",
    "\n",
    "ve_res2 = ve_solver.query(query_variables, v2_evidence)\n",
    "print(ve_res2[\"A\"])\n",
    "print(ve_res2[\"S\"])\n",
    "print(ve_res2[\"F\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using belief propagation\n",
    "\n",
    "Belief propagation should prove to be faster than variable elimination, and should return very similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pgmpy.inference import BeliefPropagation\n",
    "\n",
    "bp_solver = BeliefPropagation(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bp_solver.calibrate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒══════╤══════════╕\n",
      "│ A    │   phi(A) │\n",
      "╞══════╪══════════╡\n",
      "│ A_0  │   0.5381 │\n",
      "├──────┼──────────┤\n",
      "│ A_1  │   0.1329 │\n",
      "├──────┼──────────┤\n",
      "│ A_2  │   0.0510 │\n",
      "├──────┼──────────┤\n",
      "│ A_3  │   0.0016 │\n",
      "├──────┼──────────┤\n",
      "│ A_4  │   0.0009 │\n",
      "├──────┼──────────┤\n",
      "│ A_5  │   0.0064 │\n",
      "├──────┼──────────┤\n",
      "│ A_6  │   0.0062 │\n",
      "├──────┼──────────┤\n",
      "│ A_7  │   0.0012 │\n",
      "├──────┼──────────┤\n",
      "│ A_8  │   0.0052 │\n",
      "├──────┼──────────┤\n",
      "│ A_9  │   0.0873 │\n",
      "├──────┼──────────┤\n",
      "│ A_10 │   0.1570 │\n",
      "├──────┼──────────┤\n",
      "│ A_11 │   0.0003 │\n",
      "├──────┼──────────┤\n",
      "│ A_12 │   0.0001 │\n",
      "├──────┼──────────┤\n",
      "│ A_13 │   0.0001 │\n",
      "├──────┼──────────┤\n",
      "│ A_14 │   0.0007 │\n",
      "├──────┼──────────┤\n",
      "│ A_15 │   0.0001 │\n",
      "├──────┼──────────┤\n",
      "│ A_16 │   0.0003 │\n",
      "├──────┼──────────┤\n",
      "│ A_17 │   0.0001 │\n",
      "├──────┼──────────┤\n",
      "│ A_18 │   0.0005 │\n",
      "├──────┼──────────┤\n",
      "│ A_19 │   0.0079 │\n",
      "├──────┼──────────┤\n",
      "│ A_20 │   0.0019 │\n",
      "╘══════╧══════════╛\n",
      "╒═════╤══════════╕\n",
      "│ S   │   phi(S) │\n",
      "╞═════╪══════════╡\n",
      "│ S_0 │   0.0045 │\n",
      "├─────┼──────────┤\n",
      "│ S_1 │   0.0227 │\n",
      "├─────┼──────────┤\n",
      "│ S_2 │   0.0936 │\n",
      "├─────┼──────────┤\n",
      "│ S_3 │   0.1933 │\n",
      "├─────┼──────────┤\n",
      "│ S_4 │   0.6858 │\n",
      "╘═════╧══════════╛\n",
      "╒═════╤══════════╕\n",
      "│ F   │   phi(F) │\n",
      "╞═════╪══════════╡\n",
      "│ F_0 │   0.9955 │\n",
      "├─────┼──────────┤\n",
      "│ F_1 │   0.0042 │\n",
      "├─────┼──────────┤\n",
      "│ F_2 │   0.0002 │\n",
      "├─────┼──────────┤\n",
      "│ F_3 │   0.0000 │\n",
      "├─────┼──────────┤\n",
      "│ F_4 │   0.0000 │\n",
      "├─────┼──────────┤\n",
      "│ F_5 │   0.0000 │\n",
      "╘═════╧══════════╛\n",
      "╒══════╤══════════╕\n",
      "│ A    │   phi(A) │\n",
      "╞══════╪══════════╡\n",
      "│ A_0  │   0.5000 │\n",
      "├──────┼──────────┤\n",
      "│ A_1  │   0.2063 │\n",
      "├──────┼──────────┤\n",
      "│ A_2  │   0.0238 │\n",
      "├──────┼──────────┤\n",
      "│ A_3  │   0.0000 │\n",
      "├──────┼──────────┤\n",
      "│ A_4  │   0.0000 │\n",
      "├──────┼──────────┤\n",
      "│ A_5  │   0.0000 │\n",
      "├──────┼──────────┤\n",
      "│ A_6  │   0.0238 │\n",
      "├──────┼──────────┤\n",
      "│ A_7  │   0.0000 │\n",
      "├──────┼──────────┤\n",
      "│ A_8  │   0.0159 │\n",
      "├──────┼──────────┤\n",
      "│ A_9  │   0.0635 │\n",
      "├──────┼──────────┤\n",
      "│ A_10 │   0.0556 │\n",
      "├──────┼──────────┤\n",
      "│ A_11 │   0.0317 │\n",
      "├──────┼──────────┤\n",
      "│ A_12 │   0.0000 │\n",
      "├──────┼──────────┤\n",
      "│ A_13 │   0.0000 │\n",
      "├──────┼──────────┤\n",
      "│ A_14 │   0.0079 │\n",
      "├──────┼──────────┤\n",
      "│ A_15 │   0.0000 │\n",
      "├──────┼──────────┤\n",
      "│ A_16 │   0.0000 │\n",
      "├──────┼──────────┤\n",
      "│ A_17 │   0.0000 │\n",
      "├──────┼──────────┤\n",
      "│ A_18 │   0.0238 │\n",
      "├──────┼──────────┤\n",
      "│ A_19 │   0.0079 │\n",
      "├──────┼──────────┤\n",
      "│ A_20 │   0.0397 │\n",
      "╘══════╧══════════╛\n",
      "╒═════╤══════════╕\n",
      "│ S   │   phi(S) │\n",
      "╞═════╪══════════╡\n",
      "│ S_0 │   0.0091 │\n",
      "├─────┼──────────┤\n",
      "│ S_1 │   0.0235 │\n",
      "├─────┼──────────┤\n",
      "│ S_2 │   0.0841 │\n",
      "├─────┼──────────┤\n",
      "│ S_3 │   0.1615 │\n",
      "├─────┼──────────┤\n",
      "│ S_4 │   0.7217 │\n",
      "╘═════╧══════════╛\n",
      "╒═════╤══════════╕\n",
      "│ F   │   phi(F) │\n",
      "╞═════╪══════════╡\n",
      "│ F_0 │   0.9909 │\n",
      "├─────┼──────────┤\n",
      "│ F_1 │   0.0073 │\n",
      "├─────┼──────────┤\n",
      "│ F_2 │   0.0017 │\n",
      "├─────┼──────────┤\n",
      "│ F_3 │   0.0001 │\n",
      "├─────┼──────────┤\n",
      "│ F_4 │   0.0000 │\n",
      "├─────┼──────────┤\n",
      "│ F_5 │   0.0000 │\n",
      "╘═════╧══════════╛\n"
     ]
    }
   ],
   "source": [
    "bp_res1 = bp_solver.query(query_variables, v1_evidence)\n",
    "print(bp_res1[\"A\"])\n",
    "print(bp_res1[\"S\"])\n",
    "print(bp_res1[\"F\"])\n",
    "\n",
    "bp_res2 = bp_solver.query(query_variables, v2_evidence)\n",
    "print(bp_res2[\"A\"])\n",
    "print(bp_res2[\"S\"])\n",
    "print(bp_res2[\"F\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Bayesian model sampling\n",
    "\n",
    "Bayesian model sampling will help to shake away from possible bias in the data. This will require me to create two more functions - one for counting samples and generating a probability estimate and another for converting my evidence dictionaries to lists of States for use in the pgmpy BayesianModelSampling methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pgmpy.factors.discrete import State\n",
    "from pgmpy.sampling import BayesianModelSampling\n",
    "from pandas.core.frame import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evidence_dict_to_state_list(evidence):\n",
    "    state_list = []\n",
    "    for var_name in evidence:\n",
    "        state_list.append(State(var_name, evidence[var_name]))\n",
    "    return state_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Copied from Morgan's example\n",
    "def calcCondProb(trace, event, cond):\n",
    "    if type(trace) is DataFrame:\n",
    "        trace = trace.transpose().to_dict().values()\n",
    "    # find all samples satisfy conditions\n",
    "    for k, v in cond.items():\n",
    "        trace = [smp for smp in trace if smp[k] == v]\n",
    "    # record quantity of all samples fulfill condition\n",
    "    nCondSample = len(trace)\n",
    "    # find all samples satisfy event\n",
    "    for k, v in event.items():\n",
    "        trace = [smp for smp in trace if smp[k] == v]\n",
    "    # calculate conditional probability\n",
    "    return len(trace) / nCondSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bmsmp = BayesianModelSampling(model)\n",
    "\n",
    "nsamples = 250 #250 because 1000 took far too long\n",
    "\n",
    "bmsmp_res1 = bmsmp.rejection_sample(evidence_dict_to_state_list(v1_evidence), size=nsamples)\n",
    "bmsmp_res2 = bmsmp.rejection_sample(evidence_dict_to_state_list(v2_evidence), size=nsamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_most_likely_value(trace, event_col_name, values_to_compare):\n",
    "    high_prob = 0\n",
    "    high_value = -1\n",
    "    \n",
    "    for val in values_to_compare:\n",
    "        prob = calcCondProb(trace, {event_col_name: val}, {})\n",
    "        if prob > high_prob:\n",
    "            high_prob = prob\n",
    "            high_value = val\n",
    "    \n",
    "    return high_prob, high_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable: (Probability, Most Likely Value)\n",
      "\n",
      "Vehicle 1\n",
      "A:  (0.488, 0)\n",
      "S:  (0.656, 4)\n",
      "F:  (1.0, 0)\n",
      "\n",
      "Vehicle 2\n",
      "A:  (0.54, 0)\n",
      "S:  (0.764, 4)\n",
      "F:  (0.992, 0)\n"
     ]
    }
   ],
   "source": [
    "print(\"Variable: (Probability, Most Likely Value)\")\n",
    "\n",
    "print()\n",
    "print(\"Vehicle 1\")\n",
    "print(\"A: \", get_most_likely_value(bmsmp_res1, \"A\", range(21)))\n",
    "print(\"S: \", get_most_likely_value(bmsmp_res1, \"S\", range(5)))\n",
    "print(\"F: \", get_most_likely_value(bmsmp_res1, \"F\", range(6)))\n",
    "\n",
    "print()\n",
    "print(\"Vehicle 2\")\n",
    "print(\"A: \", get_most_likely_value(bmsmp_res2, \"A\", range(21)))\n",
    "print(\"S: \", get_most_likely_value(bmsmp_res2, \"S\", range(5)))\n",
    "print(\"F: \", get_most_likely_value(bmsmp_res2, \"F\", range(6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How I spent my time\n",
    "\n",
    "Admittedly, the above work does not look like very much. The majority of my time was spent on the data analysis notebook, which unfortunately turned out to be largely useless thanks to pgmpy's `BayesianModel.fit(pandas.DataFrame df)` method. However, I feel that I learned a lot more about how to use data for actual inference by going through all of those steps myself. Iterating through to fix floating point errors, designing a way to have a distribution of values for those subsets of possible evidence variables that did nto show up in the data, and then conforming it all to work with the library helped me better understand the libraries and the recursive nature of a lot of these Bayesian networks. So, while it was very defeating to see that pgmpy method minutes after I had finally completed all of my own code to do the same thing, I still feel I gained valuable insight.\n",
    "\n",
    "As for this notebook, I also feel that I was able to work with Bayesian networks better than I had in the previous assignment as I was able to experiment with real values and not doctor a model to do what I expected. It was quite interesting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
